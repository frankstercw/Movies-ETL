# Movies-ETL

Several assumptions are made during the collections, transformation and analysis of this data. While cleaning the data, we search for inaccurate information such as null values, inconsistent character fonts and other formatting. However, we are not reviewing the accuracy of the data itself. The first assumption we work with is operating under the idea that all information between wikipedia, kaggle and json are accurate within each data category. Knowing that anyone is capable of updating a Wikipedia page makes me question the validity of this file before we begin our analysis. Timeliness of the data is another assumption. Data categories such as ratings and revenue are "rolling data" that may be changed over time (companies may make more money as the movie continues to show/or movie reviewers may change their opninions on a show). The second assumption here is assuming that all data is taken at a specific point in time to enable comparisons and analysis. Finally, data measurement standardization is an import assumption as well. Given that we have movies that are in different languages/regions it is not impossible that revenue and other metrics are being displayed in different forms ie( dollars, euros, pesos). With this project we are assuming that all metrics are standard across each movie and category as we transform the data.
